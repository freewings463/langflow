"""
æ¨¡å—åç§°ï¼šLiteLLM æ¨¡å‹ç»„ä»¶ï¼ˆå·²åœç”¨ï¼‰

æœ¬æ¨¡å—æä¾›åŸºäº LiteLLM çš„èŠå¤©æ¨¡å‹ç»„ä»¶ï¼Œä¸»è¦ç”¨äºå°†å¤šå®¶å‚å•†æ¨¡å‹ç»Ÿä¸€ä¸º LangChain `ChatLiteLLM` æ¥å£ã€‚ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
- ç»„è£…æ¨¡å‹é…ç½®å¹¶å®ä¾‹åŒ– `ChatLiteLLM`
- å¤„ç† Azure ç­‰ Provider çš„å¿…éœ€å‚æ•°æ ¡éªŒ

å…³é”®ç»„ä»¶ï¼š
- `ChatLiteLLMModelComponent`ï¼šèŠå¤©æ¨¡å‹ç»„ä»¶

è®¾è®¡èƒŒæ™¯ï¼šå†å²ä¸Šç”¨äºæ•´åˆ LiteLLM å¤šå‚å•†æ¨¡å‹æ¥å…¥ï¼Œç°æ ‡è®°ä¸º legacyã€‚
æ³¨æ„äº‹é¡¹ï¼šä¾èµ– `litellm` åŒ…ï¼›æœªè®¾ç½®å¿…éœ€å­—æ®µæ—¶ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚
"""

from langchain_community.chat_models.litellm import ChatLiteLLM, ChatLiteLLMException

from lfx.base.constants import STREAM_INFO_TEXT
from lfx.base.models.model import LCModelComponent
from lfx.field_typing import LanguageModel
from lfx.io import (
    BoolInput,
    DictInput,
    DropdownInput,
    FloatInput,
    IntInput,
    MessageInput,
    SecretStrInput,
    StrInput,
)


class ChatLiteLLMModelComponent(LCModelComponent):
    """LiteLLM èŠå¤©æ¨¡å‹ç»„ä»¶ã€‚

    å¥‘çº¦ï¼š`model` ä¸ `provider` å¿…é¡»åŒ¹é…ï¼›Azure éœ€è¡¥é½ `api_base` ä¸ `api_version`ã€‚
    å¤±è´¥è¯­ä¹‰ï¼šä¾èµ–ç¼ºå¤±æŠ› `ChatLiteLLMException`ï¼›å‚æ•°ç¼ºå¤±æŠ› `ValueError`ã€‚
    å‰¯ä½œç”¨ï¼šé…ç½®å…¨å±€ `litellm` å‚æ•°å¹¶åˆ›å»ºæ¨¡å‹å®ä¾‹ã€‚
    """

    display_name = "LiteLLM"
    description = "`LiteLLM` collection of large language models."
    documentation = "https://python.langchain.com/docs/integrations/chat/litellm"
    icon = "ğŸš„"

    inputs = [
        MessageInput(name="input_value", display_name="Input"),
        StrInput(
            name="model",
            display_name="Model name",
            advanced=False,
            required=True,
            info="The name of the model to use. For example, `gpt-3.5-turbo`.",
        ),
        SecretStrInput(
            name="api_key",
            display_name="Chat LiteLLM API Key",
            advanced=False,
            required=False,
        ),
        DropdownInput(
            name="provider",
            display_name="Provider",
            info="The provider of the API key.",
            options=[
                "OpenAI",
                "Azure",
                "Anthropic",
                "Replicate",
                "Cohere",
                "OpenRouter",
            ],
        ),
        FloatInput(
            name="temperature",
            display_name="Temperature",
            advanced=False,
            required=False,
            value=0.7,
        ),
        DictInput(
            name="kwargs",
            display_name="Kwargs",
            advanced=True,
            required=False,
            is_list=True,
            value={},
        ),
        DictInput(
            name="model_kwargs",
            display_name="Model kwargs",
            advanced=True,
            required=False,
            is_list=True,
            value={},
        ),
        FloatInput(name="top_p", display_name="Top p", advanced=True, required=False, value=0.5),
        IntInput(name="top_k", display_name="Top k", advanced=True, required=False, value=35),
        IntInput(
            name="n",
            display_name="N",
            advanced=True,
            required=False,
            info="Number of chat completions to generate for each prompt. "
            "Note that the API may not return the full n completions if duplicates are generated.",
            value=1,
        ),
        IntInput(
            name="max_tokens",
            display_name="Max tokens",
            advanced=False,
            value=256,
            info="The maximum number of tokens to generate for each chat completion.",
        ),
        IntInput(
            name="max_retries",
            display_name="Max retries",
            advanced=True,
            required=False,
            value=6,
        ),
        BoolInput(
            name="verbose",
            display_name="Verbose",
            advanced=True,
            required=False,
            value=False,
        ),
        BoolInput(
            name="stream",
            display_name="Stream",
            info=STREAM_INFO_TEXT,
            advanced=True,
        ),
        StrInput(
            name="system_message",
            display_name="System Message",
            info="System message to pass to the model.",
            advanced=True,
        ),
    ]

    def build_model(self) -> LanguageModel:  # type: ignore[type-var]
        """æ„å»º LiteLLM æ¨¡å‹å®ä¾‹ã€‚

        å¥‘çº¦ï¼šè¿”å› `ChatLiteLLM`ï¼Œå…¶ `client.api_key` ä½¿ç”¨ç»„ä»¶è¾“å…¥ã€‚
        å¤±è´¥è¯­ä¹‰ï¼šä¾èµ–ç¼ºå¤±æŠ› `ChatLiteLLMException`ï¼›Azure å‚æ•°ç¼ºå¤±æŠ› `ValueError`ã€‚
        å‰¯ä½œç”¨ï¼šè®¾ç½® `litellm.drop_params` ä¸ `litellm.set_verbose`ã€‚

        å…³é”®è·¯å¾„ï¼ˆä¸‰æ­¥ï¼‰ï¼š
        1) å¯¼å…¥å¹¶é…ç½® `litellm`
        2) æ¸…ç†ç©ºå‚æ•°å¹¶æ ¡éªŒ Azure å¿…éœ€å­—æ®µ
        3) æ„å»ºæ¨¡å‹å®ä¾‹å¹¶æ³¨å…¥ API Key
        """
        try:
            import litellm

            litellm.drop_params = True
            litellm.set_verbose = self.verbose
        except ImportError as e:
            msg = "Could not import litellm python package. Please install it with `pip install litellm`"
            raise ChatLiteLLMException(msg) from e
        # æ³¨æ„ï¼šç§»é™¤ç©ºé”®ï¼Œé¿å…è¯·æ±‚å‚æ•°æ±¡æŸ“
        if "" in self.kwargs:
            del self.kwargs[""]
        if "" in self.model_kwargs:
            del self.model_kwargs[""]
        # æ³¨æ„ï¼šAzure provider å¿…éœ€å­—æ®µç¼ºå¤±æ—¶ç›´æ¥æŠ›é”™
        if self.provider == "Azure":
            if "api_base" not in self.kwargs:
                msg = "Missing api_base on kwargs"
                raise ValueError(msg)
            if "api_version" not in self.model_kwargs:
                msg = "Missing api_version on model_kwargs"
                raise ValueError(msg)
        output = ChatLiteLLM(
            model=f"{self.provider.lower()}/{self.model}",
            client=None,
            streaming=self.stream,
            temperature=self.temperature,
            model_kwargs=self.model_kwargs if self.model_kwargs is not None else {},
            top_p=self.top_p,
            top_k=self.top_k,
            n=self.n,
            max_tokens=self.max_tokens,
            max_retries=self.max_retries,
            **self.kwargs,
        )
        output.client.api_key = self.api_key

        return output
