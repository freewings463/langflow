"""
模块名称：cleanlab_rag_evaluator

本模块提供 Cleanlab 的 RAG 评估组件，用于评估检索上下文与回复质量。
主要功能包括：
- 功能1：评估 RAG 回复的可信度与多维度指标。
- 功能2：输出解释、其他评分以及完整评估摘要。

使用场景：对 RAG 系统输出进行可信度与检索质量诊断。
关键组件：
- 类 `CleanlabRAGEvaluator`

设计背景：将多维度 RAG 评估封装为组件，便于流程化使用。
注意事项：是否运行额外 eval 由输入开关控制，成本与时延随之变化。
"""

from cleanlab_tlm import TrustworthyRAG, get_default_evals

from lfx.custom import Component
from lfx.io import (
    BoolInput,
    DropdownInput,
    MessageTextInput,
    Output,
    SecretStrInput,
)
from lfx.schema.message import Message


class CleanlabRAGEvaluator(Component):
    """Cleanlab RAG 评估组件，用于多维度评估检索与回复质量。

    契约：输入包含 `context/query/response/api_key`；输出可信度分数与解释、其他评估与摘要。
    关键路径：
    1) 根据开关筛选评估项；
    2) 调用 `TrustworthyRAG.score` 生成评估结果；
    3) 提取分数、解释与摘要。
    异常流：网络/鉴权/评估失败会捕获并返回空结果。
    排障入口：`self.status` 记录配置与评估状态。
    决策：
    问题：RAG 质量评估维度多且配置复杂。
    方案：以组件形式封装并允许选择性评估。
    代价：增加外部 API 调用与潜在延迟。
    重评：当引入本地评估器或统一评估网关时。
    """

    display_name = "Cleanlab RAG Evaluator"
    description = "Evaluates context, query, and response from a RAG pipeline using Cleanlab and outputs trust metrics."
    icon = "Cleanlab"
    name = "CleanlabRAGEvaluator"

    inputs = [
        SecretStrInput(
            name="api_key",
            display_name="Cleanlab API Key",
            info="Your Cleanlab API key.",
            required=True,
        ),
        DropdownInput(
            name="model",
            display_name="Cleanlab Evaluation Model",
            options=[
                "gpt-4.1",
                "gpt-4.1-mini",
                "gpt-4.1-nano",
                "o4-mini",
                "o3",
                "gpt-4.5-preview",
                "gpt-4o-mini",
                "gpt-4o",
                "o3-mini",
                "o1",
                "o1-mini",
                "gpt-4",
                "gpt-3.5-turbo-16k",
                "claude-3.7-sonnet",
                "claude-3.5-sonnet-v2",
                "claude-3.5-sonnet",
                "claude-3.5-haiku",
                "claude-3-haiku",
                "nova-micro",
                "nova-lite",
                "nova-pro",
            ],
            info="The model Cleanlab uses to evaluate the context, query, and response. This does NOT need to be "
            "the same model that generated the response.",
            value="gpt-4o-mini",
            required=True,
            advanced=True,
        ),
        DropdownInput(
            name="quality_preset",
            display_name="Quality Preset",
            options=["base", "low", "medium"],
            value="medium",
            info="This determines the accuracy, latency, and cost of the evaluation. Higher quality is generally "
            "slower but more accurate.",
            required=True,
            advanced=True,
        ),
        MessageTextInput(
            name="context",
            display_name="Context",
            info="The context retrieved for the given query.",
            required=True,
        ),
        MessageTextInput(
            name="query",
            display_name="Query",
            info="The user's query.",
            required=True,
        ),
        MessageTextInput(
            name="response",
            display_name="Response",
            info="The response generated by the LLM.",
            required=True,
        ),
        BoolInput(
            name="run_context_sufficiency",
            display_name="Run Context Sufficiency",
            value=False,
            advanced=True,
        ),
        BoolInput(
            name="run_response_groundedness",
            display_name="Run Response Groundedness",
            value=False,
            advanced=True,
        ),
        BoolInput(
            name="run_response_helpfulness",
            display_name="Run Response Helpfulness",
            value=False,
            advanced=True,
        ),
        BoolInput(
            name="run_query_ease",
            display_name="Run Query Ease",
            value=False,
            advanced=True,
        ),
    ]

    outputs = [
        Output(display_name="Response", name="response_passthrough", method="pass_response", types=["Message"]),
        Output(display_name="Trust Score", name="trust_score", method="get_trust_score", types=["number"]),
        Output(display_name="Explanation", name="trust_explanation", method="get_trust_explanation", types=["Message"]),
        Output(display_name="Other Evals", name="other_scores", method="get_other_scores", types=["Data"]),
        Output(
            display_name="Evaluation Summary",
            name="evaluation_summary",
            method="get_evaluation_summary",
            types=["Message"],
        ),
    ]

    def _evaluate_once(self):
        """执行一次 RAG 评估并缓存结果。

        契约：返回评估结果字典；同一实例内复用缓存。
        关键路径：
        1) 读取开关并筛选 evals；
        2) 初始化 `TrustworthyRAG`；
        3) 调用 `score` 并缓存结果。
        异常流：评估失败时记录状态并返回空字典。
        决策：
        问题：多次访问不同输出会重复计算评估。
        方案：将评估结果缓存到 `_cached_result`。
        代价：实例生命周期内不自动刷新。
        重评：当需要实时更新或批量评估时。
        """
        if not hasattr(self, "_cached_result"):
            try:
                self.status = "Configuring selected evals..."
                default_evals = get_default_evals()
                enabled_names = []
                if self.run_context_sufficiency:
                    enabled_names.append("context_sufficiency")
                if self.run_response_groundedness:
                    enabled_names.append("response_groundedness")
                if self.run_response_helpfulness:
                    enabled_names.append("response_helpfulness")
                if self.run_query_ease:
                    enabled_names.append("query_ease")

                selected_evals = [e for e in default_evals if e.name in enabled_names]

                validator = TrustworthyRAG(
                    api_key=self.api_key,
                    quality_preset=self.quality_preset,
                    options={"log": ["explanation"], "model": self.model},
                    evals=selected_evals,
                )

                self.status = f"Running evals: {[e.name for e in selected_evals]}"
                self._cached_result = validator.score(
                    query=self.query,
                    context=self.context,
                    response=self.response,
                )
                self.status = "Evaluation complete."

            except Exception as e:  # noqa: BLE001
                self.status = f"Evaluation failed: {e!s}"
                self._cached_result = {}
        return self._cached_result

    def pass_response(self) -> Message:
        """透传原始回复。

        契约：返回 `Message(text=response)`。
        关键路径：更新状态 -> 返回消息。
        决策：
        问题：评估后仍需保留原始输出供下游使用。
        方案：提供透传输出端口。
        代价：无额外代价。
        重评：当需要附加评估元信息时。
        """
        self.status = "Passing through response."
        return Message(text=self.response)

    def get_trust_score(self) -> float:
        """返回可信度分数（0-1）。

        契约：若无结果则返回 0.0；更新状态文本。
        关键路径：读取评估结果 -> 提取 `trustworthiness.score`。
        决策：
        问题：下游需要数值型评分用于阈值判断。
        方案：单独输出评分字段。
        代价：无额外代价。
        重评：当需要输出多维评分聚合值时。
        """
        score = self._evaluate_once().get("trustworthiness", {}).get("score", 0.0)
        self.status = f"Trust Score: {score:.3f}"
        return score

    def get_trust_explanation(self) -> Message:
        """返回可信度解释文本。

        契约：若无解释返回空字符串；更新状态。
        关键路径：读取 `trustworthiness.log.explanation`。
        决策：
        问题：单一分数不足以解释评估原因。
        方案：输出解释文本供用户阅读。
        代价：解释可能较长，增加展示成本。
        重评：当需要结构化解释或多语言支持时。
        """
        explanation = self._evaluate_once().get("trustworthiness", {}).get("log", {}).get("explanation", "")
        self.status = "Trust explanation extracted."
        return Message(text=explanation)

    def get_other_scores(self) -> dict:
        """返回选中的其他评估分数。

        契约：只返回开启的 eval 项分数；空结果返回空字典。
        关键路径：读取评估结果 -> 过滤选中项 -> 汇总分数。
        决策：
        问题：非所有评估项都需要输出，需按开关过滤。
        方案：依据输入开关筛选结果。
        代价：当评估未启用时返回空。
        重评：当需要输出完整评估结构时。
        """
        result = self._evaluate_once()

        selected = {
            "context_sufficiency": self.run_context_sufficiency,
            "response_groundedness": self.run_response_groundedness,
            "response_helpfulness": self.run_response_helpfulness,
            "query_ease": self.run_query_ease,
        }

        filtered_scores = {key: result[key]["score"] for key, include in selected.items() if include and key in result}

        self.status = f"{len(filtered_scores)} other evals returned."
        return filtered_scores

    def get_evaluation_summary(self) -> Message:
        """构建评估摘要文本。

        契约：返回 `Message(text=summary)`，包含 query/context/response 与指标。
        关键路径：清理输入文本 -> 拼接指标 -> 构建摘要。
        决策：
        问题：评估结果分散，阅读成本高。
        方案：输出集中摘要便于人工审阅。
        代价：摘要为文本格式，无法结构化消费。
        重评：当需要结构化报告或可视化时。
        """
        result = self._evaluate_once()

        query_text = self.query.strip()
        context_text = self.context.strip()
        response_text = self.response.strip()

        trust = result.get("trustworthiness", {}).get("score", 0.0)
        trust_exp = result.get("trustworthiness", {}).get("log", {}).get("explanation", "")

        selected = {
            "context_sufficiency": self.run_context_sufficiency,
            "response_groundedness": self.run_response_groundedness,
            "response_helpfulness": self.run_response_helpfulness,
            "query_ease": self.run_query_ease,
        }

        other_scores = {key: result[key]["score"] for key, include in selected.items() if include and key in result}

        metrics = f"Trustworthiness: {trust:.3f}"
        if trust_exp:
            metrics += f"\nExplanation: {trust_exp}"
        if other_scores:
            metrics += "\n" + "\n".join(f"{k.replace('_', ' ').title()}: {v:.3f}" for k, v in other_scores.items())

        summary = (
            f"Query:\n{query_text}\n"
            "-----\n"
            f"Context:\n{context_text}\n"
            "-----\n"
            f"Response:\n{response_text}\n"
            "------------------------------\n"
            f"{metrics}"
        )

        self.status = "Evaluation summary built."
        return Message(text=summary)
